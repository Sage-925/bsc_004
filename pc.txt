import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report
import warnings
warnings.filterwarnings('ignore')

# Step 1: Create an imbalanced binary classification dataset
X, y = make_classification(n_samples=1000, n_features=10, n_informative=2, n_redundant=8,
                           weights=[0.9, 0.1], flip_y=0, random_state=42)

np.unique(y, return_counts=True)
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)
from imblearn.combine import SMOTETomek

smt = SMOTETomek(random_state=42)
X_train_res, y_train_res = smt.fit_resample(X_train, y_train)
np.unique(y_train_res, return_counts=True)

models = [
    (
        "Logistic Regression",
        {"C": 1, "solver": 'liblinear'},
        LogisticRegression(),
        (X_train, y_train),
        (X_test, y_test)
    ),
    (
        "Random Forest",
        {"n_estimators": 30, "max_depth": 3},
        RandomForestClassifier(),
        (X_train, y_train),
        (X_test, y_test)
    ),
    (
        "XGBClassifier",
        {"use_label_encoder": False, "eval_metric": 'logloss'},
        XGBClassifier(),
        (X_train, y_train),
        (X_test, y_test)
    ),
    (
        "XGBClassifier With SMOTE",
        {"use_label_encoder": False, "eval_metric": 'logloss'},
        XGBClassifier(),
        (X_train_res, y_train_res),
        (X_test, y_test)
    )
]
reports = []

for model_name, params, model, train_set, test_set in models:
    X_train = train_set[0]
    y_train = train_set[1]
    X_test = test_set[0]
    y_test = test_set[1]

    model.set_params(**params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    report = classification_report(y_test, y_pred, output_dict=True)
    reports.append(report)
!pip install mlflow
import mlflow
import mlflow.sklearn
import mlflow.xgboost
# Initialize MLflow
mlflow.set_experiment("Anomaly Detection")
# mlflow.set_tracking_uri("http://localhost:5000")

for i, element in enumerate(models):
    model_name = element[0]
    params = element[1]
    model = element[2]
    report = reports[i]

    with mlflow.start_run(run_name=model_name):
        mlflow.log_params(params)
        mlflow.log_metrics({
            'accuracy': report['accuracy'],
            'recall_class_1': report['1']['recall'],
            'recall_class_0': report['0']['recall'],
            'f1_score_macro': report['macro avg']['f1-score']
        })

        if "XGB" in model_name:
            mlflow.xgboost.log_model(model, name="model")
        else:
            mlflow.sklearn.log_model(model, name="model")

        # if "XGB" in model_name:
        #     model_uri = f"runs:/{run_id}/model"
        #     mlflow.register_model(model_uri=model_uri, name=model_name)

        #     print(f"Model {model_name} registered with run_id: {run_id}")

# model_uri = f"runs:/{run_id}/model"
# mlflow.register_model(model_uri=model_uri, name=model_name)
# print(f"Model {model_name} registered with run_id: {run_id}")
import mlflow
import mlflow.xgboost
from mlflow.tracking import MlflowClient

mlflow.set_experiment("Anomaly Detection")

for i, element in enumerate(models):
    model_name = element[0]
    params = element[1]
    model = element[2]
    report = reports[i]

    with mlflow.start_run(run_name=model_name) as run:
        run_id = run.info.run_id

        # Log params & metrics
        mlflow.log_params(params)
        mlflow.log_metrics({
            'accuracy': report['accuracy'],
            'recall_class_1': report['1']['recall'],
            'recall_class_0': report['0']['recall'],
            'f1_score_macro': report['macro avg']['f1-score']
        })

        # Log model
        if "XGB" in model_name:
            mlflow.xgboost.log_model(model, name="model")
        else:
            mlflow.sklearn.log_model(model, name="model")

        # Register model
        model_uri = f"runs:/{run_id}/model"
        mlflow.register_model(model_uri=model_uri, name=model_name)
        print(f"✅ Model {model_name} registered successfully with run_id: {run_id}")

client = MlflowClient()

# Source model
model_name = "XGB-Smote"
source_version = 1

# Production model name
production_model_name = "anomaly-detection-prod"

# Ensure destination model exists
try:
    client.create_registered_model(production_model_name)
except Exception:
    pass  # Already exists

# Create alias for challenger
client.set_registered_model_alias(model_name, "challenger", version=source_version)

# Copy challenger → production model
new_version = client.copy_model_version(
    src_model_uri=f"models:/{model_name}@challenger",
    dst_name=production_model_name
)

# ✅ Dynamically set "champion" alias to the latest copied version
client.set_registered_model_alias(production_model_name, "champion", version=new_version.version)

print(f"✅ {production_model_name} v{new_version.version} is now the Champion model.")

# ======================
# Load Production Model
# ======================
prod_model_uri = f"models:/{production_model_name}@champion"
loaded_model = mlflow.xgboost.load_model(prod_model_uri)

# Predict
y_pred = loaded_model.predict(X_test)
print("✅ Champion model predictions:", y_pred[:4])

